{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title \n",
    "ðŸ†“ Exercise: Q-Learning using DQN\n",
    "\n",
    "## Description\n",
    "The aim of this exercise is to implement Deep Q Networks for a pre-defined reinforcement learning environment. For this, we will be using a pre-defined environment by OpenAI Gym. We will be using an environment called FrozenLake-v0. This is the same as what was used for the previous session (Refer to it to get information about the environment).\n",
    "\n",
    "<img src=\"../fig/fig.png\" style=\"width: 500px;\">\n",
    "\n",
    "\n",
    "## Instructions\n",
    "- Initialize an environment using a pre-defined environment from OpenAI Gym.\n",
    "- Get the number of possible actions in the environment. \n",
    "- Define a simple feed-forward neural network with your choice of hidden layers and nodes.\n",
    "- Define the action sampling policy. We will be using the Epsilon Greedy policy.\n",
    "- Initialize sequential memory to store the data, which is the input to the DQN.\n",
    "- Define the DQN and compile it with Adam optimizer.\n",
    "- Fit and test the DQN model.\n",
    "\n",
    "\n",
    "## Hints\n",
    "\n",
    "<a href=\"https://gym.openai.com/docs/#environments\" target=\"_blank\">gym.make(environment-name)</a> Access a pre-defined environment\n",
    "\n",
    "Env.action_space.n : Returns the number of discrete actions\n",
    "\n",
    "Env.observation_space.n : Returns the number of discrete states\n",
    "\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\" target=\"_blank\">Dense()</a> A regular densely-connected NN layer\n",
    "\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Flatten\" target=\"_blank\">Flatten()</a> Flattens the input. \n",
    "\n",
    "<a href=\"https://keras-rl.readthedocs.io/en/latest/agents/dqn/\" target=\"_blank\">Adam()</a> Optimizer that implements the Adam algorithm\n",
    "\n",
    "<a href=\"https://keras-rl.readthedocs.io/en/latest/agents/dqn/\" target=\"_blank\">DQNAgent()</a> Initializes the DQN Agent\n",
    "\n",
    "SequentialMemory()\n",
    "\n",
    "Keras-RL provides a class called rl.memory.SequentialMemory that provides a fast and efficient data structure that we can store the agentâ€™s experiences in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this once and then comment it to ensure it does not run multiple times\n",
    "# !pip install keras-rl2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessay libraries\n",
    "import numpy as np\n",
    "import gym\n",
    "import tensorflow as tf\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation, Flatten\n",
    "# from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers.legacy import Adam\n",
    "\n",
    "tf.keras.__version__ = tf.__version__\n",
    "\n",
    "from rl.agents.dqn import DQNAgent\n",
    "from rl.policy import EpsGreedyQPolicy\n",
    "from rl.memory import SequentialMemory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initializing an environment using a pre-defined environment from OpenAI Gym \n",
    "# The environment used here is 'FrozenLake-v0'\n",
    "env = gym.make(\"FrozenLake-v1\")\n",
    "\n",
    "# Get the number of actions within the environment\n",
    "nb_actions = env.action_space.n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a Feed-Forward Neural Network \n",
    "\n",
    "# Initialize a keras sequential model\n",
    "model = Sequential()\n",
    "\n",
    "# Flatten the input to have an input shape of (1,) + \n",
    "# shape of the environment state space i.e. the observation space\n",
    "model.add(Flatten(input_shape = (1,2)))\n",
    "\n",
    "# Add Dense layers with Relu activation\n",
    "# The number of hidden layers and number of nodes in each layer is your choice\n",
    "\n",
    "model.add(Dense(128, activation = \"relu\"))\n",
    "model.add(Dense(256, activation = \"relu\"))\n",
    "model.add(Dense(128, activation = \"relu\"))\n",
    "\n",
    "# Add an output layer with number of nodes as the number of actions\n",
    "model.add(Dense(nb_actions))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the policy to sample the actions\n",
    "# We will be using the Epsilon-Greedy algorithm\n",
    "policy = EpsGreedyQPolicy()\n",
    "# implements an epsilon-greedy strategy. It means that with probability epsilon (Îµ), it explores randomly by selecting a random action, \n",
    "# and with probability (1 - Îµ), it exploits by selecting the action with the highest estimated Q-value (action-value) for the current state.\n",
    "\n",
    "# To store our data initialize Sequential Memory with limit=500000 and window_length of 1\n",
    "memory = SequentialMemory(limit = 500000, window_length = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **DQN AGENT**\n",
    "\n",
    "<img src=\"./images/dqn.png\" alt=\"DQN Agent\" style=\"width:700px\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 20:50:34.336472: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_27/bias/Assign' id:3030 op device:{requested: '', assigned: ''} def:{{{node dense_27/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_27/bias, dense_27/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training for 5000 steps ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-18 20:50:34.570657: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_27/BiasAdd' id:3035 op device:{requested: '', assigned: ''} def:{{{node dense_27/BiasAdd}} = BiasAdd[T=DT_FLOAT, _has_manual_control_dependencies=true, data_format=\"NHWC\"](dense_27/MatMul, dense_27/BiasAdd/ReadVariableOp)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2023-09-18 20:50:34.622187: W tensorflow/c/c_api.cc:304] Operation '{name:'count_25/Assign' id:3301 op device:{requested: '', assigned: ''} def:{{{node count_25/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](count_25, count_25/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "float() argument must be a string or a real number, not 'dict'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ting/2021-CS109B/content/lectures/lecture33/notebook/ai4_s3_dqn.ipynb Cell 8\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ting/2021-CS109B/content/lectures/lecture33/notebook/ai4_s3_dqn.ipynb#X10sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m dqn\u001b[39m.\u001b[39mcompile(optimizer\u001b[39m=\u001b[39mAdam(learning_rate\u001b[39m=\u001b[39m\u001b[39m1e-3\u001b[39m), metrics\u001b[39m=\u001b[39m[\u001b[39m\"\u001b[39m\u001b[39mmse\u001b[39m\u001b[39m\"\u001b[39m])\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ting/2021-CS109B/content/lectures/lecture33/notebook/ai4_s3_dqn.ipynb#X10sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m \u001b[39m# Fit the DQN by passing with environment with nb_steps as 5000\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ting/2021-CS109B/content/lectures/lecture33/notebook/ai4_s3_dqn.ipynb#X10sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m \u001b[39m# You have an option to visualize the output, which is done by implicitly calling the render function of the environment\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ting/2021-CS109B/content/lectures/lecture33/notebook/ai4_s3_dqn.ipynb#X10sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m \u001b[39m# However, this will slow down the training process and is not recommended for EdStem\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ting/2021-CS109B/content/lectures/lecture33/notebook/ai4_s3_dqn.ipynb#X10sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m \u001b[39m# To see the complete training details, set verbose as 2\u001b[39;00m\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ting/2021-CS109B/content/lectures/lecture33/notebook/ai4_s3_dqn.ipynb#X10sZmlsZQ%3D%3D?line=15'>16</a>\u001b[0m dqn\u001b[39m.\u001b[39mfit(env, nb_steps\u001b[39m=\u001b[39m\u001b[39m5000\u001b[39m, verbose\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m)\n",
      "File \u001b[0;32m~/ENTER/envs/cs109a/lib/python3.11/site-packages/rl/core.py:168\u001b[0m, in \u001b[0;36mAgent.fit\u001b[0;34m(self, env, nb_steps, action_repetition, callbacks, verbose, visualize, nb_max_start_steps, start_step_policy, log_interval, nb_max_episode_steps)\u001b[0m\n\u001b[1;32m    165\u001b[0m callbacks\u001b[39m.\u001b[39mon_step_begin(episode_step)\n\u001b[1;32m    166\u001b[0m \u001b[39m# This is were all of the work happens. We first perceive and compute the action\u001b[39;00m\n\u001b[1;32m    167\u001b[0m \u001b[39m# (forward step) and then use the reward to improve (backward step).\u001b[39;00m\n\u001b[0;32m--> 168\u001b[0m action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mforward(observation)\n\u001b[1;32m    169\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    170\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocessor\u001b[39m.\u001b[39mprocess_action(action)\n",
      "File \u001b[0;32m~/ENTER/envs/cs109a/lib/python3.11/site-packages/rl/agents/dqn.py:224\u001b[0m, in \u001b[0;36mDQNAgent.forward\u001b[0;34m(self, observation)\u001b[0m\n\u001b[1;32m    221\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, observation):\n\u001b[1;32m    222\u001b[0m     \u001b[39m# Select an action.\u001b[39;00m\n\u001b[1;32m    223\u001b[0m     state \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmemory\u001b[39m.\u001b[39mget_recent_state(observation)\n\u001b[0;32m--> 224\u001b[0m     q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_q_values(state)\n\u001b[1;32m    225\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining:\n\u001b[1;32m    226\u001b[0m         action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpolicy\u001b[39m.\u001b[39mselect_action(q_values\u001b[39m=\u001b[39mq_values)\n",
      "File \u001b[0;32m~/ENTER/envs/cs109a/lib/python3.11/site-packages/rl/agents/dqn.py:68\u001b[0m, in \u001b[0;36mAbstractDQNAgent.compute_q_values\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_q_values\u001b[39m(\u001b[39mself\u001b[39m, state):\n\u001b[0;32m---> 68\u001b[0m     q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_batch_q_values([state])\u001b[39m.\u001b[39mflatten()\n\u001b[1;32m     69\u001b[0m     \u001b[39massert\u001b[39;00m q_values\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_actions,)\n\u001b[1;32m     70\u001b[0m     \u001b[39mreturn\u001b[39;00m q_values\n",
      "File \u001b[0;32m~/ENTER/envs/cs109a/lib/python3.11/site-packages/rl/agents/dqn.py:63\u001b[0m, in \u001b[0;36mAbstractDQNAgent.compute_batch_q_values\u001b[0;34m(self, state_batch)\u001b[0m\n\u001b[1;32m     61\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mcompute_batch_q_values\u001b[39m(\u001b[39mself\u001b[39m, state_batch):\n\u001b[1;32m     62\u001b[0m     batch \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprocess_state_batch(state_batch)\n\u001b[0;32m---> 63\u001b[0m     q_values \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\u001b[39m.\u001b[39mpredict_on_batch(batch)\n\u001b[1;32m     64\u001b[0m     \u001b[39massert\u001b[39;00m q_values\u001b[39m.\u001b[39mshape \u001b[39m==\u001b[39m (\u001b[39mlen\u001b[39m(state_batch), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnb_actions)\n\u001b[1;32m     65\u001b[0m     \u001b[39mreturn\u001b[39;00m q_values\n",
      "File \u001b[0;32m~/ENTER/envs/cs109a/lib/python3.11/site-packages/keras/src/engine/training_v1.py:1321\u001b[0m, in \u001b[0;36mModel.predict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   1318\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m(inputs)\n\u001b[1;32m   1320\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_make_predict_function()\n\u001b[0;32m-> 1321\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpredict_function(inputs)\n\u001b[1;32m   1323\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(outputs) \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1324\u001b[0m     \u001b[39mreturn\u001b[39;00m outputs[\u001b[39m0\u001b[39m]\n",
      "File \u001b[0;32m~/ENTER/envs/cs109a/lib/python3.11/site-packages/keras/src/backend.py:4587\u001b[0m, in \u001b[0;36mGraphExecutionFunction.__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   4583\u001b[0m         \u001b[39m# We need to do array conversion and type casting at this level,\u001b[39;00m\n\u001b[1;32m   4584\u001b[0m         \u001b[39m# since `callable_fn` only supports exact matches.\u001b[39;00m\n\u001b[1;32m   4585\u001b[0m         tensor_type \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mas_dtype(tensor\u001b[39m.\u001b[39mdtype)\n\u001b[1;32m   4586\u001b[0m         array_vals\u001b[39m.\u001b[39mappend(\n\u001b[0;32m-> 4587\u001b[0m             np\u001b[39m.\u001b[39masarray(value, dtype\u001b[39m=\u001b[39mtensor_type\u001b[39m.\u001b[39mas_numpy_dtype)\n\u001b[1;32m   4588\u001b[0m         )\n\u001b[1;32m   4590\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_dict:\n\u001b[1;32m   4591\u001b[0m     \u001b[39mfor\u001b[39;00m key \u001b[39min\u001b[39;00m \u001b[39msorted\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfeed_dict\u001b[39m.\u001b[39mkeys()):\n",
      "\u001b[0;31mTypeError\u001b[0m: float() argument must be a string or a real number, not 'dict'"
     ]
    }
   ],
   "source": [
    "# Initialize the DQNAgent with the neural network model, nb_actions as the number of actions in the environment, \n",
    "# set the memory as the sequential memory defined above, nb_steps_warmup as 100, policy as the epsilon greedy policy defined above\n",
    "# and set the target_model_update as 1e-2\n",
    "\n",
    "# Deep Q-Network (DQN) agent\n",
    "# rl.agents.dqn.DQNAgent(model, policy=None, test_policy=None, enable_double_dqn=True, enable_dueling_network=False, dueling_type='avg')\n",
    "dqn = DQNAgent(model, policy = policy, nb_actions = nb_actions, memory = memory,nb_steps_warmup = 100, target_model_update = 1e-2 )\n",
    "\n",
    "# Compile the DQN with Adam optimizer with learning rate of 1e-3 and metric as mse\n",
    "dqn.compile(optimizer=Adam(learning_rate=1e-3), metrics=[\"mse\"])\n",
    "\n",
    "# Fit the DQN by passing with environment with nb_steps as 5000\n",
    "# You have an option to visualize the output, which is done by implicitly calling the render function of the environment\n",
    "# However, this will slow down the training process and is not recommended for EdStem\n",
    "# To see the complete training details, set verbose as 2\n",
    "dqn.fit(env, nb_steps=5000, verbose=2);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test your model by passing the environment and running for 10 episodes\n",
    "dqn.test(env, nb_episodes=10)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
