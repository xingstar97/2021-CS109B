{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title \n",
    "Exercise: Attention\n",
    "\n",
    "## Description :\n",
    "In this exercise, you will implement an Attention mechanism. We load three encoder hidden states into `enc_states`, and 1 decoder hidden state into `dec_state`. Your task is to compute the final `context_vector`.\n",
    "\n",
    "That is, you should calculate an Attention score for every encoder hidden state, exponentiate these, then normalize them so they sum to 1. These are your Attention weights. Then, produce a context vector by multiplying each Attention weight by its corresponding encoder hidden state."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">**REMINDER**</font>: After running every cell, be sure to auto-grade your work by clicking 'Mark' in the lower-right corner. Otherwise, no credit will be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports useful libraries\n",
    "import math\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">YOU DO NOT NEED TO EDIT THE CELL BELOW</font>\n",
    "\n",
    "The follow code loads three encoder states into the dictionary `enc_states`, whereby the keys are 0, 1, and 2, and their respective values are lists of 50 floats (representing each hidden state). The code also populates a single list of floats, `dec_state`, which contains 50 floats (representing the hidden state)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'hidden_states.txt'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ting/2021-CS109B/content/lectures/lecture24/notebook/L24_Ex1_student.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ting/2021-CS109B/content/lectures/lecture24/notebook/L24_Ex1_student.ipynb#W4sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m             dec_state \u001b[39m=\u001b[39m [\u001b[39mfloat\u001b[39m(t) \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m line\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m \u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m1\u001b[39m:]]\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/ting/2021-CS109B/content/lectures/lecture24/notebook/L24_Ex1_student.ipynb#W4sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39mreturn\u001b[39;00m enc_states, dec_state\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/ting/2021-CS109B/content/lectures/lecture24/notebook/L24_Ex1_student.ipynb#W4sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m enc_states, dec_state \u001b[39m=\u001b[39m load_hidden_states(\u001b[39m\"\u001b[39m\u001b[39mhidden_states.txt\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "\u001b[1;32m/home/ting/2021-CS109B/content/lectures/lecture24/notebook/L24_Ex1_student.ipynb Cell 5\u001b[0m line \u001b[0;36m6\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ting/2021-CS109B/content/lectures/lecture24/notebook/L24_Ex1_student.ipynb#W4sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m enc_states \u001b[39m=\u001b[39m {}\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ting/2021-CS109B/content/lectures/lecture24/notebook/L24_Ex1_student.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m dec_state \u001b[39m=\u001b[39m []\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ting/2021-CS109B/content/lectures/lecture24/notebook/L24_Ex1_student.ipynb#W4sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m f \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39m(filename)\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ting/2021-CS109B/content/lectures/lecture24/notebook/L24_Ex1_student.ipynb#W4sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mfor\u001b[39;00m line \u001b[39min\u001b[39;00m f\u001b[39m.\u001b[39mreadlines():\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ting/2021-CS109B/content/lectures/lecture24/notebook/L24_Ex1_student.ipynb#W4sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m     model, num \u001b[39m=\u001b[39m line\u001b[39m.\u001b[39msplit()[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m_\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/ENTER/envs/cs109a/lib/python3.11/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'hidden_states.txt'"
     ]
    }
   ],
   "source": [
    "# assumes we're passing in several enc states but only 1 dec states\n",
    "def load_hidden_states(filename):\n",
    "    enc_states = {}\n",
    "    dec_state = []\n",
    "    \n",
    "    f = open(filename)\n",
    "    for line in f.readlines():\n",
    "        model, num = line.split()[0].split(\"_\")\n",
    "        if model == \"enc\":\n",
    "            enc_states[int(num)] = [float(t) for t in line.split(\" \")[1:]]\n",
    "        else:\n",
    "            dec_state = [float(t) for t in line.split(\" \")[1:]]\n",
    "    return enc_states, dec_state\n",
    "\n",
    "enc_states, dec_state = load_hidden_states(\"hidden_states.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">YOU DO NOT NEED TO EDIT THE CELL BELOW</font>\n",
    "\n",
    "The follow code simply computes the attention score as the dot-product between the two passed-in embeddings. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculates the attention score as the dot product\n",
    "def calculate_attention_score(v1, v2):\n",
    "    return sum(a*b for a, b in zip(v1, v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, populate `attention_scores` with the _exponentiated_ attention scores: $e^{(\\text{score(enc_i, dec_j)})}$. The main aspect to figure out is which hidden states to pass to `calculate_attention_score()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'enc_states' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/ting/2021-CS109B/content/lectures/lecture24/notebook/L24_Ex1_student.ipynb Cell 9\u001b[0m line \u001b[0;36m4\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ting/2021-CS109B/content/lectures/lecture24/notebook/L24_Ex1_student.ipynb#X11sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m attention_scores \u001b[39m=\u001b[39m []\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ting/2021-CS109B/content/lectures/lecture24/notebook/L24_Ex1_student.ipynb#X11sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m):\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ting/2021-CS109B/content/lectures/lecture24/notebook/L24_Ex1_student.ipynb#X11sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m     ai \u001b[39m=\u001b[39m np\u001b[39m.\u001b[39mexp(calculate_attention_score(enc_states[i], dec_state))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ting/2021-CS109B/content/lectures/lecture24/notebook/L24_Ex1_student.ipynb#X11sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m     attention_scores\u001b[39m.\u001b[39mappend(ai)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'enc_states' is not defined"
     ]
    }
   ],
   "source": [
    "### edTest(test_a) ###\n",
    "attention_scores = []\n",
    "for i in range(3):\n",
    "    ai = np.exp(calculate_attention_score(enc_states[i], dec_state))\n",
    "    attention_scores.append(ai)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, simply _normalize_ each of the exponentiated scores and store them in `attention_weights`. They should sum to 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_b) ###\n",
    "attention_weights = []\n",
    "total = np.sum(np.array(attention_scores))\n",
    "for i in attention_scores:\n",
    "    weight = attention_scores[i]/total\n",
    "    attention_weights.append(weight)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, create the final context vector `context_vector`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_c) ###\n",
    "\n",
    "# YOUR CODE HERE\n",
    "\n",
    "context_vector = np.sum(np.array(attention_weights) * np.array(enc_states), axis = 1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
