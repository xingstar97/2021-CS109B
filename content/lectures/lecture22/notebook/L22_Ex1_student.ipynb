{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title :\n",
    "Unigram  LM\n",
    "\n",
    "## Description :\n",
    "Text data is unlike the typical \"design matrix\", i.i.d. data that we've often worked with. Here, you'll gain practice working with actual words, as you'll parse, count, and calculate a probability.\n",
    "\n",
    "An individual unigram's likelihood (**unsmoothed**) is defined as:\n",
    "\n",
    "$$L\\left(w\\right)=\\frac{n_w\\left(D_t\\right)}{n_o\\left(D_t\\right)}$$\n",
    "\n",
    "where the numerator represents the number of times word $w$ appeared in the training corpus $D_t$.\n",
    "\n",
    "For this exercise, we will define the **smoothed** unigram's likelihood as:\n",
    "\n",
    "$$L\\left(w\\right)=\\frac{n_w\\left(D_t\\right)\\ +\\alpha}{n_o\\left(D_t\\right)\\ +\\alpha\\left|V\\right|}$$\n",
    "\n",
    "where $\\alpha$ is a specified real-valued number (doesn't have to be an integer), and $|V|$ is the cardinality of the lexicon (i.e., the number of distinct word types in the vocabulary)\n",
    "\n",
    "The likelihood of a new sequence $H$ is simply defined by the likelihood of each token, multiplied by each other:\n",
    "\n",
    "$$L\\left(H\\right)=\\prod_{w\\ \\in H}^{ }L\\left(w\\right)$$\n",
    "\n",
    "## HINTS :\n",
    "Depending on your approach, these functions could help you:\n",
    "\n",
    "- `re.sub()` (regular expression)\n",
    "- `.split()`\n",
    "- `.lower()`\n",
    "- `.strip()`\n",
    "- `.replace()`\n",
    "- `.sum()`\n",
    "- `defaultdict` data structure\n",
    "- `Counter` data structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">**REMINDER**</font>: After running every cell, be sure to auto-grade your work by clicking 'Mark' in the lower-right corner. Otherwise, no credit will be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports some libraries you might find useful\n",
    "import re\n",
    "import math\n",
    "from collections import Counter\n",
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# necessary for our experiments\n",
    "training_file = \"../data/ex1_train.txt\"\n",
    "dev_file = \"../data/ex1_dev.txt\"\n",
    "punctuation = ['.', '!', '?']\n",
    "\n",
    "sample1 = \"I love data science!\"\n",
    "sample2 = \"I love NLP!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `parse_string()` which takes as input a string (e.g., the contents of a file). It should return this text as a list of tokens. Specifically, the tokens should:\n",
    "- be lowercased\n",
    "- be separated by whitespace and any character present in the list of `punctuation`.\n",
    "- include no trailing or preceeding whitespace (none of the returned tokens should be of white space or empty)\n",
    "\n",
    "For example, if the input is **\" I   LOVE daTa!!\"**, it should return **[\"i\", love\", \"data\", \"!\", \"!\"]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'.|!|?| '"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"|\".join(punctuation+[\" \"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\\\.|!|\\\\?|\\\\ '"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'|'.join(map(re.escape, punctuation+[\" \"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i', 'love', 'data', 'science', 'i', 'love', 'computer', 'science']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### edTest(test_a) ###\n",
    "def parse_string(text):\n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    text = text.lower().strip()\n",
    "    sep = punctuation+[\" \"]\n",
    "    sep = '|'.join(map(re.escape, sep))\n",
    "    #  The re.escape() function is used to escape any special characters in a string so that they can be safely used in a regular expression pattern. \n",
    "    # For example, if one of your separators is a character with a special meaning in regular expressions (e.g., . or *), \n",
    "    # re.escape() will ensure it's treated as a literal character and not as part of a regular expression pattern.\n",
    "    text = [s for s in re.split(sep, text) if s.strip()]\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return text\n",
    "\n",
    "# DO NOT EDIT THE LINES BELOW\n",
    "text = open(training_file).read()\n",
    "tokens = parse_string(text)\n",
    "tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `count_tokens()` that takes a list of tokens and simply outputs a dictionary-style count of the items. For example, if the input is **['run', 'forrest', 'run']**, it should return a `dict`, `defaultdict`, or `Counter` with 2 keys: **{'run':2, 'forrest':1}** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'i': 2, 'love': 2, 'data': 1, 'science': 2, 'computer': 1}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### edTest(test_b) ###\n",
    "def count_tokens(tokens): \n",
    "    \n",
    "    # YOUR CODE STARTS HERE\n",
    "    word_counts = {}\n",
    "    for word in tokens:\n",
    "        if word in word_counts:\n",
    "            word_counts[word] +=1\n",
    "        else:\n",
    "            word_counts[word] = 1\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return word_counts\n",
    "\n",
    "# DO NOT EDIT THIS LINE\n",
    "word_counts = count_tokens(tokens)\n",
    "word_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `calculate_likelihood()` that takes `tokens` (a list of strings) and `word_counts` (dictionary-type) and returns the likelihood of the sequence of tokens. You will run your function with the tokens parsed from the `sample1` string. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.001953125"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### edTest(test_c) ###\n",
    "def calculate_likelihood(tokens, word_counts):\n",
    "    total_likelihood = 1\n",
    "    \n",
    "    total = 0\n",
    "    for word in word_counts:\n",
    "        total += word_counts[word]\n",
    "    # YOUR CODE STARTS HERE\n",
    "    for token in tokens:\n",
    "        total_likelihood *= word_counts[token]/total\n",
    "    # YOUR CODE ENDS HERE\n",
    "        \n",
    "    return total_likelihood\n",
    "\n",
    "# DO NOT EDIT THE LINES BELOW\n",
    "sample1_tokens = parse_string(sample1)\n",
    "likelihood = calculate_likelihood(sample1_tokens, word_counts)\n",
    "likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function `calculate_smoothed_likelihood()` that is the same as the previous function but includes a smoothing parameter `alpha`. Again, you should return the likelihood of the sequence of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(word_counts.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.001928208925293473 0.002576788291074005\n"
     ]
    }
   ],
   "source": [
    "### edTest(test_d) ###\n",
    "\n",
    "def calculate_smoothed_likelihood(alpha, tokens, word_counts):\n",
    "\n",
    "    total_likelihood = 1\n",
    "\n",
    "    # YOUR CODE STARTS HERE\n",
    "    total = 0\n",
    "    for word in word_counts:\n",
    "        total += word_counts[word]\n",
    "    for token in tokens:\n",
    "        if token not in word_counts:\n",
    "            word_counts[token] = 0\n",
    "        total_likelihood *= (word_counts[token]+alpha)/(total + alpha * len(word_counts.keys()))\n",
    "    # YOUR CODE ENDS HERE\n",
    "    return total_likelihood\n",
    "\n",
    "# DO NOT EDIT THE LINES BELOW\n",
    "sample1_tokens = parse_string(sample1)\n",
    "sample1_likelihood = calculate_smoothed_likelihood(0.5, sample1_tokens, word_counts)\n",
    "\n",
    "sample2_tokens = parse_string(sample2)\n",
    "sample2_likelihood = calculate_smoothed_likelihood(0.5, sample2_tokens, word_counts)\n",
    "\n",
    "print(sample1_likelihood, sample2_likelihood)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
