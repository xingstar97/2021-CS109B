{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title \n",
    "Exercise: Self-Attention\n",
    "\n",
    "## Description :\n",
    "In this exercise, you will implement a Self-Attention Head for the **3rd word** of a 4-word input. From a Pickled file, we load `query`, `key`, and `value` vectors that corresponds to 4 different inputs (thus, a total of 12 vectors). Specifically, this is loaded into 3 respective `dicts`.\n",
    "\n",
    "You only need to calculate the final `z` \"context\" vector that corresponds to the 3rd word (i.e., `z2`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### <font color=\"red\">**REMINDER**</font>: After running every cell, be sure to auto-grade your work by clicking 'Mark' in the lower-right corner. Otherwise, no credit will be given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports useful libraries\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">YOU DO NOT NEED TO EDIT THE CELL BELOW</font>\n",
    "\n",
    "The follow code loads the `queries`, `keys`, and `values` vectors that correspond to 4 distinct words. Here, all vectors have a length of 25. Each of these variables (e.g., `queries`) is a `dict`, indexed by the word number. For example `queries[2]` corresponds to the 3rd word (it's 0-indexed), and its value is a list of length 25, which corresponds to the actual _query_ vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'L25.p'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/home/ting/2021-CS109B/content/lectures/lecture25/notebook/L25_Ex1_student.ipynb Cell 5\u001b[0m line \u001b[0;36m1\n\u001b[0;32m----> <a href='vscode-notebook-cell:/home/ting/2021-CS109B/content/lectures/lecture25/notebook/L25_Ex1_student.ipynb#W4sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m pickled_content \u001b[39m=\u001b[39m pickle\u001b[39m.\u001b[39mload(\u001b[39mopen\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mL25.p\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m))\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ting/2021-CS109B/content/lectures/lecture25/notebook/L25_Ex1_student.ipynb#W4sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m queries, keys, values \u001b[39m=\u001b[39m [pickled_content[i] \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39m3\u001b[39m)]\n\u001b[1;32m      <a href='vscode-notebook-cell:/home/ting/2021-CS109B/content/lectures/lecture25/notebook/L25_Ex1_student.ipynb#W4sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# to illustrate, let's print the query, key, and value vectors that correspond to teh 3rd word in the sentence:\u001b[39;00m\n",
      "File \u001b[0;32m~/ENTER/envs/cs109a/lib/python3.11/site-packages/IPython/core/interactiveshell.py:284\u001b[0m, in \u001b[0;36m_modified_open\u001b[0;34m(file, *args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[39mif\u001b[39;00m file \u001b[39min\u001b[39;00m {\u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m}:\n\u001b[1;32m    278\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    279\u001b[0m         \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mIPython won\u001b[39m\u001b[39m'\u001b[39m\u001b[39mt let you open fd=\u001b[39m\u001b[39m{\u001b[39;00mfile\u001b[39m}\u001b[39;00m\u001b[39m by default \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    280\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mas it is likely to crash IPython. If you know what you are doing, \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    281\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39myou can use builtins\u001b[39m\u001b[39m'\u001b[39m\u001b[39m open.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    282\u001b[0m     )\n\u001b[0;32m--> 284\u001b[0m \u001b[39mreturn\u001b[39;00m io_open(file, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'L25.p'"
     ]
    }
   ],
   "source": [
    "pickled_content = pickle.load(open(\"L25.p\", \"rb\"))\n",
    "queries, keys, values = [pickled_content[i] for i in range(3)]\n",
    "\n",
    "# to illustrate, let's print the query, key, and value vectors that correspond to teh 3rd word in the sentence:\n",
    "print(\"query vector for 3rd word:\", queries[2])\n",
    "print(\"\\nkey vector for 3rd word:\", keys[2])\n",
    "print(\"\\nvalue vector for 3rd word:\", values[2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font color=\"red\">YOU DO NOT NEED TO EDIT THE CELL BELOW</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# returns the dot product of two passed-in vectors\n",
    "def calculate_dot_product(v1, v2):\n",
    "    return sum(a*b for a, b in zip(v1, v2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, populate the `self_attention_scores` list by calculating the **four** Attention scores that correspond to the **3rd word**. Each Attention score should be divided by $\\sqrt{d_k}$ (where $d_k$ represents the length of the key vector), and the ordering should be natural. That is, the 1st item in `self_attention_scores` should correspond to the Attention score for the 1st word, the 2nd item in `self_attention_scores` should correspond to the Attention score for 2nd word, and so on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_a) ###\n",
    "self_attention_scores = []\n",
    "d = len(keys)\n",
    "for i in range(4):\n",
    "    score = calculate_dot_product(queries[2], keys[i])/d\n",
    "    self_attention_scores.append(score)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, populate the `softmax` list by calculating the softmax of each of the four Attention scores found in the previous cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_b) ###\n",
    "softmax_scores = []\n",
    "total = np.sum(np.exp(np.array(self_attention_scores)))\n",
    "for i in self_attention_scores:\n",
    "    score = np.exp(self_attention_scores[i])/total\n",
    "    softmax_scores.append(score)\n",
    "# YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the cell below, create the final $z2$ list that corresponds to the 3rd word. $z2$ should have a length of 25."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_c) ###\n",
    "z2 = np.sum(np.array(values) * np.array(softmax_scores), axis =1)\n",
    "\n",
    "\n",
    "\n",
    "# YOUR CODE HERE"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
