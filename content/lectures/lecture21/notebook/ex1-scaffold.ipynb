{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Title :\n",
    "LSTM v/s GRU\n",
    "\n",
    "## Description :\n",
    "The goal of this exercise is to compare the performance between two popular gating methods, i.e LSTM and GRUs:\n",
    "\n",
    "<img src=\"../fig/fig.png\" style=\"width: 500px;\">\n",
    "\n",
    "## Instructions :\n",
    "- Read the IMDB dataset from the helper code given.\n",
    "- Take a quick look at your training inputs and labels.\n",
    "- Pad the values to a fix number `max_words` in-order to have sequences of the same size.\n",
    "- Build, compile and fit a GRU model\n",
    "- Evaluate the model performance on the test set and report the test set accuracy.\n",
    "- Again build, compile and fit a model but use the LSTM architecture instead. \n",
    "- Evaluate the LSTM model's performance on the test set and report the test set accuracy.\n",
    "- Compare the performance of all the two models.\n",
    "\n",
    "## Hints:\n",
    "\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Embedding\" target=\"_blank\">tf.keras.layers.Embedding()</a> Turns positive integers (indexes) into dense vectors of fixed size.\n",
    "\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/LSTM\" target=\"_blank\">tf.keras.layers.LSTM()</a> Long Short-Term Memory layer - Hochreiter 1997.\n",
    "\n",
    "<a href=\"https://www.tensorflow.org/api_docs/python/tf/keras/layers/Dense\" target=\"_blank\">tf.keras.layers.Dense()</a> Just your regular densely-connected NN layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM\n",
    "\n",
    "We will use both GRU and LSTM to perform sentiment analysis in tensorflow.keras and compare their performance using the custom IMDB dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-07 22:20:04.310432: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-07 22:20:04.336826: I tensorflow/tsl/cuda/cudart_stub.cc:28] Could not find cuda drivers on your machine, GPU will not be used.\n",
      "2023-08-07 22:20:04.337460: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-08-07 22:20:04.934070: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.layers import RNN\n",
    "from tensorflow.keras.datasets import imdb\n",
    "from tensorflow.keras.models import Model,Sequential\n",
    "from tensorflow.keras.layers import Input,Dense,LSTM,GRU,Embedding\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from prettytable import PrettyTable\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We use the same dataset as the previous exercise \n",
    "# with open('imdb_mini.pkl','rb') as f:\n",
    "    # X_train, y_train, X_test, y_test = pickle.load(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/ting/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to /home/ting/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv(\"IMDB Dataset.csv\")\n",
    "\n",
    "import re\n",
    "\n",
    "def process(x):\n",
    "    x = re.sub('[,\\.!?:()\"]', '', x)\n",
    "    x = re.sub('<.*?>', ' ', x)\n",
    "    x = re.sub('http\\S+', ' ', x)\n",
    "    # \\S+: matches one or more non-whitespace characters\n",
    "    x = re.sub('[^a-zA-Z0-9]', ' ', x)\n",
    "    # matches any character that is not (^ negation) an uppercase letter (A-Z), a lowercase letter (a-z), or a digit (0-9).\n",
    "    x = re.sub('\\s+', ' ', x)\n",
    "    # \\s: matches any whitespace character, such as spaces, tabs, and newline characters.\n",
    "    return x.lower().strip()\n",
    "\n",
    "data['review'] = data['review'].apply(lambda x: process(x))\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt')\n",
    "\n",
    "sw_set = set(nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "def sw_remove(x):\n",
    "    words = nltk.tokenize.word_tokenize(x)\n",
    "    filtered_list = [word for word in words if word not in sw_set]\n",
    "    return ' '.join(filtered_list)\n",
    "\n",
    "data['review'] = data['review'].apply(lambda x: sw_remove(x))\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "train, test = train_test_split(data, test_size = 0.2, random_state=109)\n",
    "\n",
    "X_train = train[\"review\"]\n",
    "y_train = 1 * (train[\"sentiment\"] == \"positive\")\n",
    "X_test = test[\"review\"]\n",
    "y_test = 1*(test[\"sentiment\"] == \"positive\")\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "\n",
    "dict_size = 5000\n",
    "tokenizer = Tokenizer(num_words=dict_size)\n",
    "tokenizer.fit_on_texts(data['review'])\n",
    "\n",
    "train_rev_tokens = tokenizer.texts_to_sequences(X_train)\n",
    "test_rev_tokens = tokenizer.texts_to_sequences(X_test)\n",
    "seq_lengths =  np.array([len(sequence) for sequence in train_rev_tokens])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Similar to the previous exercise, we will pre-preprocess our review sequences\n",
    "# We fix the vocabulary size to 5000 because our custom \n",
    "# dataset was curated with that\n",
    "vocabulary_size = 5000\n",
    "# Max word length for each review will be 500\n",
    "max_words = 200\n",
    "# we set the embedding size to 32\n",
    "embedding_size=32\n",
    "# Pre-padding sequences to max_words lenth\n",
    "X_train = sequence.pad_sequences(train_rev_tokens, maxlen=max_words,padding='pre')\n",
    "X_test = sequence.pad_sequences(test_rev_tokens, maxlen=max_words,padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We create the mapping between words and sequences\n",
    "word2id = imdb.get_word_index()\n",
    "# We need to adjust the mapping by 3 because of tensorflow.keras preprocessing\n",
    "# more here: https://stackoverflow.com/questions/42821330/restore-original-text-from-keras-s-imdb-dataset\n",
    "word2id = {k:(v+3) for k,v in word2id.items()}\n",
    "word2id[\"<PAD>\"] = 0\n",
    "word2id[\"<START>\"] = 1\n",
    "word2id[\"<UNK>\"] = 2\n",
    "word2id[\"<UNUSED>\"] = 3\n",
    "\n",
    "# Reversing the key,value pair will give the id2word\n",
    "id2word = {i: word for word, i in word2id.items()}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚è∏ For this problem with `embedding_size=32` ($X_t$) and `hidden_size=100` ($H_{t-1}$), how many trainable weights are associated with the GRU Cell (assuming `use_bias=True`)?\n",
    "\n",
    "\n",
    "#### A. 39600\n",
    "#### B. 39800\n",
    "#### C. 40200\n",
    "#### D. 40400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow1) ###\n",
    "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
    "answer1 = 'C'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 15:03:06.045866: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "# Comparing with GRU model\n",
    "embedding_size=32\n",
    "hidden_size = 100\n",
    "gru_model=Sequential()\n",
    "# Add Embedding, GRU and a Dense layer \n",
    "# Add Embedding layer with vocabulary_size, embedding_size and input_length\n",
    "# Add GRU with hidden_size\n",
    "# Add Dense layer with 1 unit and sigmoid activation\n",
    "gru_model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "gru_model.add(GRU(hidden_size))\n",
    "gru_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "gru_model.compile(loss='binary_crossentropy',optimizer = 'Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 200, 32)           160000    \n",
      "                                                                 \n",
      " gru (GRU)                   (None, 100)               40200     \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 200301 (782.43 KB)\n",
      "Trainable params: 200301 (782.43 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "gru_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow2) ###\n",
    "gru_cnt_params = gru_model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n",
      "  1/157 [..............................] - ETA: 20s - loss: 0.2224 - accuracy: 0.9219"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 16:25:09.961027: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 32000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 23s 144ms/step - loss: 0.2086 - accuracy: 0.9185\n",
      "Epoch 2/3\n",
      "157/157 [==============================] - 22s 143ms/step - loss: 0.1852 - accuracy: 0.9289\n",
      "Epoch 3/3\n",
      "157/157 [==============================] - 22s 139ms/step - loss: 0.1674 - accuracy: 0.9371\n",
      "313/313 [==============================] - 4s 12ms/step - loss: 0.3003 - accuracy: 0.8853\n",
      "Model accuracy on the test set is 0.89\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "num_epochs = 3\n",
    "gru_model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs)\n",
    "gru_score = gru_model.evaluate(X_test,y_test)\n",
    "print(f'Model accuracy on the test set is {gru_score[1]:.2f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ‚è∏ For this problem with `embedding_size=32` ($X_t$) and `hidden_size=100` ($H_{t-1}$), how many trainable weights are associated with the LSTM Cell (assuming `use_bias=True`)?\n",
    "\n",
    "\n",
    "#### A. 52800\n",
    "#### B. 53200\n",
    "#### C. 54200\n",
    "#### D. 51400"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "53200"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(32 + 100) * 100 * 4 + 100 * 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow3) ###\n",
    "# Submit an answer choice as a string below (eg. if you choose option A, put 'A')\n",
    "answer2 = 'A'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comparing with LSTM model\n",
    "embedding_size=32\n",
    "hidden_size = 100\n",
    "\n",
    "lstm_model=Sequential()\n",
    "\n",
    "# Add Embedding, LSTM and a Dense layer \n",
    "# Add Embedding layer with vocabulary_size, embedding_size and input_length\n",
    "# Add LSTM with hidden_size\n",
    "# Add Dense layer with 1 unit and sigmoid activation\n",
    "lstm_model.add(Embedding(vocabulary_size, embedding_size, input_length=max_words))\n",
    "lstm_model.add(LSTM(100))\n",
    "lstm_model.add(Dense(1, activation='sigmoid'))\n",
    "\n",
    "lstm_model.compile(loss='binary_crossentropy',optimizer = 'Adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding_1 (Embedding)     (None, 200, 32)           160000    \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 100)               53200     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 101       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 213301 (833.21 KB)\n",
      "Trainable params: 213301 (833.21 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "lstm_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow4) ###\n",
    "lstm_cnt_params = lstm_model.count_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "213301"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lstm_cnt_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-08-10 16:11:33.029495: W tensorflow/tsl/framework/cpu_allocator_impl.cc:83] Allocation of 32000000 exceeds 10% of free system memory.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "157/157 [==============================] - 27s 166ms/step - loss: 0.4546 - accuracy: 0.7897\n",
      "Epoch 2/3\n",
      "157/157 [==============================] - 26s 164ms/step - loss: 0.2467 - accuracy: 0.9018\n",
      "Epoch 3/3\n",
      "157/157 [==============================] - 26s 164ms/step - loss: 0.2130 - accuracy: 0.9159\n",
      "313/313 [==============================] - 5s 16ms/step - loss: 0.2680 - accuracy: 0.8911\n",
      "Model accuracy on the test set is 0.89\n"
     ]
    }
   ],
   "source": [
    "batch_size = 256\n",
    "num_epochs = 3\n",
    "lstm_model.fit(X_train, y_train, batch_size=batch_size, epochs=num_epochs)\n",
    "lstm_score = lstm_model.evaluate(X_test,y_test)\n",
    "print(f'Model accuracy on the test set is {lstm_score[1]:.2f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-------------------+\n",
      "| Strategy | Test set accuracy |\n",
      "+----------+-------------------+\n",
      "| GRU RNN  |       88.94%      |\n",
      "| LSTM RNN |       89.11%      |\n",
      "+----------+-------------------+\n"
     ]
    }
   ],
   "source": [
    "# Finally, we compare the results from the three implementations above\n",
    "\n",
    "pt = PrettyTable()\n",
    "pt.field_names = [\"Strategy\",\"Test set accuracy\"]\n",
    "pt.add_row([\"GRU RNN\",f'{gru_score[1]*100:.2f}%'])\n",
    "pt.add_row([\"LSTM RNN\",f'{lstm_score[1]*100:.2f}%'])\n",
    "print(pt)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üç≤ Which variant is better, LSTM or GRU?\n",
    "\n",
    "Both LSTM & GRUs solve the vanishing gradient problem of RNNs but each has their advantages and disadvantages. (Read [this](https://arxiv.org/pdf/1412.3555v1.pdf) paper for a thorough analysis of the two methods)\n",
    "Based on your understanding, which architecture is more appropriate for the current analysis?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### edTest(test_chow5) ###\n",
    "# Type your answer within in the quotes given\n",
    "answer3 = 'LSTM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
